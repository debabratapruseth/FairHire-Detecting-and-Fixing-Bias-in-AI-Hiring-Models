# FairHire: Detecting and Fixing Bias in AI Hiring Models
A hands-on notebook to detect, visualize, and mitigate gender bias in hiring models using Fairlearn and scikit-learn.

---

## Project Overview
This notebook demonstrates how to **detect and mitigate bias** in a simulated hiring dataset.  
We use a simple dataset containing candidate experience, gender, and shortlisting decisions to explore fairness in machine learning.

---

## Goal
To identify whether **gender bias** exists in hiring decisions  
and to show how **fairness-aware ML techniques** (like *Fairlearn*) can help reduce it.

---

## Steps Covered
1️⃣ **Load and explore the dataset** – Check for data imbalance and gender-based patterns.  
2️⃣ **Train two models** – One including gender as a feature, and one excluding it.  
3️⃣ **Compare predictions** – Visualize and interpret potential gender bias.  
4️⃣ **Measure fairness** – Use *Disparate Impact Ratio (DIR)* and *Fairlearn* metrics.  
5️⃣ **Mitigate bias** – Apply *Fairlearn’s Demographic Parity* constraint and evaluate improvements.

---

## Tools Used
- **pandas, matplotlib, seaborn** → Data analysis and visualization  
- **scikit-learn** → Logistic Regression and model evaluation  
- **fairlearn** → Fairness metrics and bias mitigation (Demographic Parity)

---

## Output Highlights
- Bias detection through model comparison  
- Fairness metrics dashboards  
- Visual insights into model decisions  
- Bias mitigation results with Fairlearn

---

## Learnings
- Even simple models can inherit bias from data.  
- Removing sensitive features doesn’t always eliminate discrimination.  
- Fairness constraints help balance accuracy and equality.

---

**If you find this project useful, give it a star!**  
Let’s make AI fairer — one model at a time.

---

### Keywords
machine learning bias, fairness in AI, gender bias detection, fairlearn tutorial, responsible AI, bias mitigation, disparate impact, demographic parity, scikit-learn fairness, ethical AI demo, explainable AI, data ethics
